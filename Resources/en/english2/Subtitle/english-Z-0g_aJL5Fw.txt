"Design TikTok. Hey everyone, I'm here today with Adam, and today we'll be doing a mock system design interview. I'm really excited to have Adam in this video. Adam, do you mind just introducing yourself and telling us a little bit about what you work on?

Sure! Hi, my name is Adam. I work on cloud engineering and back-end systems currently at Oracle, but I'll be moving over to Google soon.

Awesome! Yeah, we're really excited to jump into a practice system design interview today. The question that we'll be doing is design TikTok.

Okay, cool! Design TikTok. Well, this is a phone. I actually have never used TikTok. Great! So maybe we can start by just quickly—if you could give me a—I have an idea, obviously I know what it is—but if you could just kind of give me a quick overview of what we're looking for, and maybe be a little bit specific on the portion of TikTok we're wanting to focus on.

Awesome, yeah! So, a high-level overview of TikTok is it's a mobile app for video sharing between users. The basic idea is you're able to upload a video to TikTok, and then you're able to view a feed of videos. So you see one video at a time, you scroll up, you see the next video. You can follow users, so you see the videos that they post, and then you can also perform actions on those videos, like favoriting them or commenting on them. I think those are sort of the basic things we'll want to support.

Cool! All right, that makes sense. I'm sorry, I'm just going to take a minute and kind of jot down some notes real quick.

Sounds great! In the meantime, I'll switch over and just screen share here into our whiteboard.

Cool! So, it looks like TikTok is a mobile app. If it's okay with you, I think I'm just gonna focus on sort of the back-end infrastructure and not so much go into the mobile app portion, just because I think there's a little bit more feature-rich detail there.

The back-end system?

Yeah, sounds great! From what I can see, the first thing I kind of want to dive into are sort of these functional requirements. When I say functional requirements, I just kind of mean the sort of top-level buckets of work that we're wanting to look into. The first thing from what you explained to me was upload videos, right? So, a user from their mobile device—or really, in theory, since we're just diving into the back end—if it's okay, I'm going to assume that we're just going to create an API that is sort of client agnostic that's gonna accept user data to upload videos. So, we're gonna kind of think sort of section one here. If I was gonna write this up, the functional—I'm just gonna say number one here is upload videos.

And so, when we upload videos, these videos are sort of time-boxed, right? It’s sort of the same way as Instagram; they only are about 15 seconds each, is that right?

Yeah, and yeah, we can assume they're, you know, 30 seconds to maybe a maximum of like a minute long.

Okay, so max one minute. Cool! And I'm also assuming that if I'm uploading videos, there would also be text associated with that—like with Instagram or Facebook, where you can upload a photo, but you can also tag and add text and things like that?

Yeah, that's a good question! Yeah, let's assume you can add a comment or caption to it. We don't need to get maybe into the details of tagging and how that would work, but yeah, maybe some text data associated with it.

Okay, cool! Upload—I deleted what I was writing here—upload, so let's say video plus text. Cool! And then kind of number two here, what I'm thinking is you said view feed. So, when I think about viewing the feed, what we're doing is we're aggregating—and correct me if I'm wrong—we're talking about aggregating videos from, is it people I follow only? Is it people I follow plus—sort of like I know that TikTok has this special algorithm that they use to grab recommended videos. Would it be kind of like—can I assume there's sort of a mixture of those two things?

Yeah, I think that's a good question! For simplicity, we could focus maybe on videos of people you follow, but I would be interested in hearing about sort of how you would create a trending or video recommendation as well. For the purposes of this, do you want me to include the ability for me to comment on videos as part of that functionality?

Yeah, let's try to build out how we would like favorite videos and maybe follow particular creators.

Okay, cool! So, yeah, that's actually my next functional requirement here, which is kind of follow users. So, favoriting, following, commenting, forwarding—I don't know if they allow that, but I'll just kind of bucket that as like a sort of video interaction endpoint.

Sure! Cool! All right, so that kind of covers these as like the general functional requirements. Really quickly, I just want to talk about these non-functional requirements—so really non-functional, what I mean by that is kind of like let's start with availability, latency, and scale. 

So, it sounds to me like I'm gonna make an assumption here that this needs to be a highly available system, just simply because of the scale of the users that are going to be using it. It sounds like it's going to be a lot, and really, because it is serving videos, it does need to be highly available. So, I'm just going to say roughly, you know, highly available—I’m going to say around 99.999. If that's correct, that sounds about right for you?

Yeah, that sounds great! Because, like, you know, there are always these trade-offs, right? Like when we talk about when I'm thinking about how I'm going to design a system, I'm just trying to understand if it’s not super high—like if it maybe doesn't need to be as highly available, maybe there are certain things we can do to balance our budget and how we're going to budget for our compute resources and that sort of thing.

So, also think about latency. The one good thing is, from the sound of it, since it is a mobile device, it sounds like we can kind of cache a lot of the content on the device itself. So when we initially pull stuff, we'll be able to obviously get the top-level stuff really quickly, but after that, we can kind of be pulling stuff in the background. So, it sounds like we have a little bit of leeway there, does that sound about right to you?

Yeah, that sounds good! I think maybe later on we can talk a little bit about the latency or how this would differ across different features, like upload versus download, and how to think about that.

Yeah, okay, I'll just put TBD for that. Cool! And then, scale—can you just give me a rough estimate on like if we're talking about users? Do you have any kind of idea how many users we're talking about here, like in a day?

Yeah, in a day, let's say we want to support like a million active users in a day.

Okay, so a million daily active users. Cool! So, I'm just going to do a couple of estimates here now that we're talking about this. So, this gives me an idea of how I'm going to store everything. So, we said max here for the videos was about—let's see—one minute. I'm just going to make a quick assumption and say that a minute of compressed H.264 video is about five megabytes. That sounds about right to you?

I think it sounds like, you know, and then maybe we can say each user is uploading two per day, equals 10 megabytes per day, per user. I'm not going to get too much into this yet; I just want to get a rough idea for the big chunk here, which is videos. The rest of it, user metadata, is going to be minimal. I think it's going to be, you know, 1k per user per day. I think that's a pretty good estimate roughly.

Cool! Okay, cool! So, I think with all that, is there anything else I'm kind of—you can see that I'm missing here? Does that seem like a rough good sort of overview of the system that we're trying to tackle here?

Yeah, this seems like a good overview. I think we can dive in now.

Okay! Cool! So, I think I'm gonna start with just kind of a couple of API endpoints just to wrap my head around. So, I think what I'd like to start with is—we'll just start here and we'll say upload video, just because we have it really. That's going to be—and if I talk about—I don't get too much of the parameters of it, but if I talk about the database schema that sort of backs that, really our user object is going to be something along the lines of we’re going to have some sort of user ID, and that's going to be like a UID. We're going to have some sort of video link—actually, I'll get to that later—but I think that's going to be—once we upload the video, we're going to want to store, I think, in some sort of blob storage like S3, so this will be a fully qualified link to that object. This would be like a URL, and then maybe meta—this would be just like metadata. This would just be string data. So, that's kind of the first endpoint I'm thinking. 

So, let me just kind of go into that real quick. I'm not going to get into much of the user detail stuff here just because I think that's sort of a solved problem. I think we can get to that if we have time, but for the scope of this, I'm not really going to talk too much about it. So, I'm just gonna take a minute and kind of think this through.

Okay, cool! So, I think, obviously, we're gonna want some sort of database, right, to back this, and this is gonna be where we store this table. So, I'm assuming this is going to—for right now, I'm just going to make an assumption this is going to be a relational database, Postgres, whatever flavor or relational database you so choose. So, upload video is going to send this object to the relational database. Could you just briefly sort of talk about maybe the differences between a relational database versus another type of database and why we might want to use this?

Yeah, so like a relational database versus like a NoSQL database, for example, a relational database is going to be a little bit more structured. Typically, you'll use relational databases for things like user data objects linking different tables together. For example, you can have a single user that has many videos, so you would have that many video objects, and those can be stored in two different tables, and you can do SQL queries against those. NoSQL databases are really good for unstructured data, like log data, things like that, so a little bit more free-form in nature, not as structured in the sense that you're going to be querying that data and doing a lot of joins to it. You're kind of just going to be more like free-form searching for keys and values inside that data. So, in this case, you know, a relational database can be a lot more strict, but it can also be more space and speed-efficient for specific queries. 

So, yeah, I think that makes sense here. So, yeah, I think what we do is upload the video object to this database. Oh, sorry, rather, this would actually just be the video data, right? It would be the best way to describe this—the video. I just say the video table, right? So, this is going to send to the video table. The idea here is that this is just the data, because what we're then going to want to do is send the actual video object to some sort of cloud bucket like I mentioned before. So, like S3, just use this cloud thing. So, like, this would be like a blob store. The idea here is that, yeah, the video itself actually lives here, and then in this table, we have a link to that. So, we're going to be running like a POST—oops! We're going to be running a POST to this API endpoint, this upload video endpoint. So, I'm running out of space; let me just move all this over a bit. 

This would be my video plus my user information. So, yeah, the upload video accepts the video and the user information; we save it in the table, we upload it to blob storage, and then we're going to return like some sort of, you know, some sort of 200 response, which is what the API would respond with to the app itself, saying that the video went through correctly. So, that kind of handles our upload section. It's pretty straightforward for the upload section. Is there any questions about that, or did you have anything I think I missed there?

No, I think that makes a lot of sense. Yeah, we have the sort of overview of where the video goes, where the metadata goes, and how the app would sort of make that request, so I think that makes sense.

Cool! Awesome! So, let's talk really quick about the view feed. So, this will be kind of a similar endpoint, view feed, and what this is actually going to be is a GET request. So, when you open up the app—sorry—so when you open up the app, I think, would we want to load this content right when the application opens? Like, I'm guessing we want to start pre-loading as much as we can ahead of time so that the user isn't waiting too much for videos to load, right?

Yeah, I think we'd want to do that, insofar as it would let us view videos faster, but maybe not so much that it would use up user bandwidth and things like that. 

Yeah, I think maybe like the top—like whatever the first three videos, for example—we'd want to just grab those as quick as possible because the reason I asked that is I think it would make sense to sort of have some sort of Redis cache over here—some sort of cache, whatever it is. The idea here would be that we actually preload a list of like user—the top 10 videos that we're going to load for the user before they even get to the view feed page. So, like, if a user with a specific user UUID hits this view feed, we're going to go to this cache that's already pre-built, and we're going to grab those top videos that are already pre-selected. So, like, for example, this UUID would give me—this would respond with like, you know, 10 videos—video links, rather—that are associated with that user ID and the links for the blob storage so that the app would then be grabbing them from the blob stores like right away. Does that make sense?

Yeah, that makes sense! Yeah, that's super interesting! Can you tell me a little bit more about how the cache would work?

Yeah, so I'm actually thinking about a service here—in the background—that would run. So this would sort of be like a pre-cache service, and this would just kind of run on like a schedule, but also just like maybe on demand—something like that. What it would do is compile playlists for users and pre-cache them, so it could be potentially like we could sort of base it on when the user actually goes and does the GET request that we sort of preload it for the next one, or we just do it in the background. There are a couple of strategies we could use there. What I'm trying to get away from is relying too much because, from what I can tell, this system seems very read-heavy, right? Like there's a lot—there's gonna be a lot of reading going on. So, in addition to having this main database, which I'll get to scaling, I think in a bit, but I think for this main database, we're also going to want to have some sort of read worker, which would be like a read-only database. The reason being we don't want to create too much load on this single database that's accepting these uploads—we want to have something that's managing the reads. So, in fact, I would actually do something like this where the secondary is pulling from the primary, and this is used for read-only, which builds the pre-cache, which loads the cache, which then when the view feed gets hit, it loads it instantly, right? Yeah, at least the query, right? So, does that make sense?

Yeah, that makes a lot of sense! I think getting back to one of the first questions about latency, can you talk a little bit about how introducing this cache would sort of affect latency in the system and seeing updates and things like that?

Yeah, for sure! So, one thing we would need to consider is how quickly we want—like if I don't know if actually the video that I upload, I don't think that would show up in my feed. 

And if so...

Yeah, I think what—so this feed is basically a curated list of videos. So, I think the point here is that—because I know that TikTok has like a special algorithm they use to populate your feed, so I'm trying to get rid of all that happening at the moment of GET—like at the moment of loading the app. I'm trying to have it pre-built, right? So that right away, we're getting what we need from the back end and not having to wait for some sort of service to compile that on the fly. Plus, this also solves a little bit of our scaling issue because if we have situations where—if you can imagine TikTok has a million users—if what if one million users all get on at the same time and run the same exact query? We could really kill our databases because we're running all these queries at the same time. We can also up that latency, so you can solve that with some auto-scaling groups, but even auto-scaling groups take time to sort of spin up, so that's kind of the thing I'm thinking about here. But the real key here is—I think I just noticed that this seems very read-heavy, so I was trying to get to the point where those reads were sort of managed on their own.

Yeah, I think that's a great insight! Cool! I think that kind of handles the view feed for the most part. Yeah, I feel pretty good about that. 

The last piece here would be favoriting videos. I think we would have just another endpoint here where we would put another one in there. 

Yeah, so I think we have another one, and this would just be like—I’m just going to call this user activity, maybe, or something like that. This would be—that's a little bit generic, but I'm going to speed through this. We're in a little bit of long time. I try and name it something better in the real world, like something more descriptive of what it actually did. But the idea here is that when I hit the user activity, this is like follow—this is doing liking videos, that sort of thing. And I apologize, I'm actually going to just move this around a little bit so that I can get to my database cleaner.

Yeah, yeah! Just move this down a bit. So, yeah, the idea behind user activity is literally just going to hit this RDP—the database. And so for my—I'd actually have a different table here. I'd have like some sort of—let me just put this over here, put it right here for now. So this table would look something along the lines of—still have a user ID and a UUID. I think we would have—so, yeah, I think this would be like a followers—a user activity table. And the idea here would be—we're going to need a couple of keys here. So following is actually going to have to be a foreign key to another table because we're going to have—for any one user, we're going to obviously be following multiple other accounts, right? So we're going to need another table in there that's got a list of user IDs that I'm following—user accounts. In addition to that, we're going to want likes, so we're going to want to store another—I think, another foreign key to like a videos table. So I'm assuming we like—assuming if each account has a like a table of videos like we're seeing up here, we would basically have—sorry, I think I missed something here—we're probably going to want to have a video UUID, like a video ID, and this would be a UID field here so that I can key to this video ID and say that that's a list of the likes that I'm liking. And then this all feeds later into the pre-cache service algorithm, which I'm not going to get into, but I think that's important to store. So basically, whatever activity I have—all this user activity is going to hit the database. I'm going to store this up here for now; it's going to hit this database and add that to my—to this table, right? So, I'm now following this user, I'm liking this video, etc. So that if I ever needed to like run a GET request—which sounds like the pre-cache service we'll actually need to run—some sort of request against user activity or some sort of—maybe we have like an internal service that manages that; I'm not sure. But yeah, I'd have an API that would return all the users' likes and followers, essentially, is what I'm getting at. And maybe that's a GET request here from the user activity, but that's sort of the rough idea of what I'm thinking for that. Does that make sense from sort of a flow perspective?

Yeah, that makes sense! Awesome! Okay, well, yeah, I think we've got sort of the main interactions here. We've got most of the features sort of structurally built out. I'd be curious as sort of a follow-up here, what do you think would be sort of the bottlenecks of the system if you were to, for example, like 10x the traffic one day or something like that to really scale things up?

Yeah, absolutely! So, first thing I have to think about is just regions. If we're thinking about having multiple versions of this in regions—like regional data centers—we might want to consider that, just geolocating. So my first inclination is just whatever user—we're going to put the user behind some sort of—basically, all these API endpoints are going to want to go behind some sort of CDN, right? Content Delivery Network. So like Akamai or something like that because the minute we grab a video, for example—let's say—in a lot of situations where we really 10x traffic, it's typically in these scenarios around like, you know, somebody famous sends out a video, right? And they have 10 million followers, and everyone wants to view the video. So the idea behind the CDN is the minute the first person grabs it, the CDN caches it locally behind the CDN system so that all the users who are grabbing it are just grabbing it from the CDN, and the CDN is doing like, you know, a routing to the local—the closest node so that the internet traffic isn't always hitting my system, my blob storage; it's only hitting the CDN. So that sort of fronts—because like, obviously, these videos are relatively big in the scale of like—in the site when we're talking about 10 million users times, you know, five megabytes per video—that's a pretty substantial number to deal with. So, putting a CDN behind that—in front of that—is important. 

I think having a load balancer in front of these API endpoints as well will do a couple of things. Number one, it's going to let me commit to this highly available non-functional requirement because these API endpoints can be scalable, and we can do things like have multiple deployments—so multiple services running. So this, the load balancer is picking which one of these services is available at any one time and routing traffic accordingly, so A or B, for example. So, it's not only is it balancing between the two, but it also lets you do things like zero downtime deployment. So, if we have to update this software, the back-end software that's running this, we would flip all traffic to B and only allow B to be serving traffic while A is down and getting updates, right? A to B is a very simple, simplistic view; that could be hundreds of compute instances, but that's sort of the idea there. 

So, that's kind of where I see the bottleneck. Like I said, the database is always kind of a bottleneck, I think. So, we'd have definitely like sort of a main write database with read-only workers that manage sort of the GET requests, and these would also be in some sort of auto-scaling group so that we could scale them up as needed. Same thing with the pre-cache service. I think we have a pretty good idea once we got more in detail with this how our pre—like what we needed and what—as far as timing and requirements around what we need for—we'd have a pretty good understanding. So, I don't know how auto-scaling this would have to be; it might just be able to be like set in stone or roughly equivalent to that so that we know ahead of time. 

Caches—similar situation. Caches are pretty scalable and things like that. The one last thing I think about is this write database. So, if we were to 10x traffic to this, we'd want to consider doing some sort of database sharding so that we are—and we'd have some sort of database shard service here in front of this where—where we are basically—sorry, let me just clean this up a bit—basically, so this sharding service sits in front of the write, the—all the writes, and it picks—it's like a load balancer for databases, essentially—and it picks which database it's going to go to based on—there are all kinds of sharding algorithms or strategies you can do. Things like the simplest version of that is like if you took maybe like a region, so all US requests go to this database, all UK requests go to this database—you can do things like that.

Yeah, so that's kind of the way I would picture it because that will help us to split the load between these databases.

Yeah, awesome! Well, I think we've got—we jumped into some details there. I think overall we've got all the pieces sort of in place, and I think this is a pretty good stopping part. Do you have any sort of last things you would add to this before we sort of wrap up?

No, I think just that something I might—if I had a little more time, I'd maybe get a little bit more into the pre-cache service because I think it might need its own almost separate database structure and things like that. But I think, like I said, I don't want to get too much into the weeds with the TikTok-specific like other algorithmic kind of stuff. I think just kind of keeping it saying that this exists somewhere and we would just use it is good enough for the purposes of this interview. So, no, I feel pretty good about it.

Awesome! All right, well, yeah, let's call it here. And yeah, so just sort of recapping what we went over. So, we talked about designing the API, we talked about database structure, we talked about, you know, different microservices and load balancing and things like that. I thought overall, like, you did a really awesome job of really clearly communicating all these ideas and also, you know, sort of jumping in and providing extra knowledge about the specific things. Even when I asked like a follow-up question or even without asking, I feel like you really jumped in and sort of showed you had mastery over all of these topics, which was really awesome. And even though this is sort of a little bit of a condensed version of this, I feel like you could have gone multiple layers deeper on any of these topics, so it really shows me that you know what you're talking about. But yeah, what did you think overall?

Yeah, I think it's always interesting trying to design something that you've never used, so we're sort of having to make a couple of assumptions on, you know, knowing—I know Facebook, I know Instagram, so I kind of understand those—but making some assumptions on how it's used by people really kind of helped. But yeah, I thought it went well.

Well, thank you so much, Adam, and good luck to everyone on your system design interview!"