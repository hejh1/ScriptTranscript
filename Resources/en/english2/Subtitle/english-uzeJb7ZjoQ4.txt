How do you design a real-time messaging app that can support millions of users? [Music] In this video, we'll be talking about how to design and build an application like Facebook Messenger, WhatsApp, Discord, or Slack. We'll be discussing the high-level architecture of these systems as well as some specific features like how to build real-time messaging, group messaging, image and video uploads, as well as push notifications. Now, we'll also be talking about some best practices along the way about how to build and scale a reliable system.

Now, before we dive into some specific features and components of our system, let's talk a little bit about some of the high-level features and goals that we want to implement. Some important product goals that we want to support obviously are real-time messaging from one individual to another, as well as group messaging that can support multiple users. We want to support maybe something like an online status to show whether a user is currently online or offline, as well as supporting image and video uploads in addition to text messages. We also may want to support some extra bells and whistles like read receipts or push notifications, so we'll get to those if we have time.

We also want to keep in mind some technical goals and constraints here, thinking about how to build a low-latency system to support real-time messaging and what applications or technologies we need to choose to make that happen. We also want to build something that can support a really high volume of requests, so millions of users potentially writing messages at the same time. We also want to make sure, of course, that our system is highly reliable and available, and we want to ensure that our messaging system is secure and doesn't result in users being sent messages they shouldn't receive.

All right, great! So now that we've laid out some of the features we want to support, let's talk about the overall architecture of this app. Specifically, let's discuss how we're going to send messages from one user to another. Let's pretend we have two users here, and we want to send a message between them. The reason we need to use a chat server application in this case is that, on the internet, it's actually really hard to establish a direct connection from one user to another and to make that connection reliable. In addition, we also want to support things like storing and retrieving message histories later. So we need to have our chat server here to establish and broker this connection and store these messages so they can be retrieved from different devices later in time.

I'm going to go ahead and draw in our chat API server. We also need to establish a connection from users to this chat server. Now, let's think about what happens when User A sends a message to User B. What we want to happen is that the user sends a message to the server, and the server relays that message instantly to the user that it's intended for. However, this kind of breaks the model of how HTTP requests work on the internet because they can't actually be server-initiated; they have to be client-initiated. So something about this isn't going to work, and we're going to have to come up with something else.

Now, I've got a few options in mind, so I'm going to talk about those and discuss their trade-offs. The first one we can do is something called HTTP polling. In this model, instead of just sending one request to the server, we're going to repeatedly ask the server if there's any new information available. Most of the time, the server is going to reply with "No, there's no new information available," and then once in a while, it'll say, "Hey, yeah, I received a new message for you," and it will reply with that. For a variety of reasons, this is probably not the right solution for this problem because it means we're going to be sending a lot of unnecessary requests to our server, and it also means that we're going to have pretty high latency. We're only going to be able to receive messages when we ask for them, not when they're actually received by the server.

The second option we have is something called long polling. In this model, we're still using a traditional HTTP request, but instead of resolving immediately with the result, we're actually going to have the server hold on to the request and wait until data is available before it replies with the result. In this way, we maintain an open connection with the server at all times, and then once data is sent back, we immediately request a new connection and keep that open until data is available. This is a little bit better because it solves our latency problems somewhat, and we don't have to create all these unnecessary requests all the time. However, we have to maintain this open connection, and if there's lots of data coming from the server, it means we still have to initiate a new request to get the next piece of data. While it's good for some systems, like notifications and things like that, it's probably not the best for a real-time chat application like we're trying to build here.

The third option we have is something called WebSockets, and this is the solution I'm going to recommend because it was designed for this application. In WebSockets, we still maintain an open connection with the server, but instead of being just a one-way connection, it's actually a full-duplex connection. Now we can send data to the server, and the server can push data to us. This connection is maintained and kept open for the duration of the session. 

This architecture presents some unique challenges for us because there are some practical limitations to how many open connections a server can have at one time. WebSockets is built on the TCP protocol, which has about 16 bits for the port number. This means there's a real limitation of about 65,000 connections that any one server can have open at a time. So instead of having one API server, we're obviously going to need to have a lot of servers to handle all of these WebSocket connections, and we're going to need a load balancer or gateway sitting in front of them to help balance these connections and route them to the correct server. I'm going to go ahead and replace this with a load balancer and draw in some API servers here instead.

All right, great! I'm just sort of visualizing this with, you know, about three servers. But if we're trying to support millions or hundreds of millions of users and we can only support thousands of requests per server, that means we're going to have hundreds or thousands of these servers serving requests and keeping these connections open. Our system is going to have to reach a pretty massive scale here. 

In addition, we now have a new problem because before, we were able to send a message from one user to another via our chat API server. However, now we have a distributed system, and we need to be able to communicate from one API server to another. In fact, we've created another messaging problem here because these API servers need to know how to talk to each other. 

One model we could adopt here, one design pattern we can use, is something like a pub/sub message queue. That fits nicely into this problem because it's a natural solution for a messaging problem between servers in a distributed system. I'm going to go ahead and draw in a message service here, which is going to implement this message queue. The idea is that each API server will publish messages into this centralized queue and subscribe to updates for the users that it's connected to. That way, when a new message comes in, it can be added to the queue, and any service that's listening for messages for that user can then receive that update and forward the message to the user.

Now we still need to think about how we're going to store and persist these messages in our database and how to model this relationship between messages and users. So let's go ahead and draw in a database and think about how that's going to work. When we think about what kind of database to choose for our application, let's think back to the beginning where we set out our requirements for the system. We know we want to support a really large volume of requests and store a lot of messages. We also care a lot about the availability and uptime of our service, so we want to pick a database that fits these requirements. 

We know from things like the CAP theorem that there are universal trade-offs between principles like consistency, availability, and the ability to partition or shard our database. We want to focus on this ability to shard and partition and keep our database available rather than things like consistency, which are less important in a messaging application than they would be in something like a financial application. With that in mind, I think I would choose something like a NoSQL database that has built-in replication and sharding abilities—something like Cassandra or HBase would be great for this application. 

All right, so now let's talk about how we're going to store and model this data in our database. We know we're going to need a few key tables and features like users, messages, and then we're also going to probably need this concept of conversations, which will be groups of users who are supposed to receive messages. I'm going to go ahead and draw those tables in. 

Starting with the users table, we're going to have a unique ID for this user. We're also going to have something like a username or a name for them to display. Then we're probably also going to want to have something like a last active timestamp. The idea here is that this would allow us to support features like online status or being able to see when a user was last online. This would just be a simple timestamp that would be the date of their last activity.

Next, we're going to need a messages table, which is going to have a unique ID but will also store a reference to the user who made the message. We're also going to have a reference or an ID for the conversation that it belongs to. We'll talk more about that in a moment, but then obviously, at the end, we're also going to need the text of the message. If we want to support something like media uploads, like images or videos, we also want to store something like a media URL here in our database. This won't be the actual data, but it'll be the URL where the user can access this data to download it.

Next, we're going to need this conversations table that I was talking about. The idea here is that this would simply just be an ID and perhaps something like a name. In the case of an application like Slack or Discord, this could be the channel name of the conversation, so that's sort of an optional string that we could store here.

Great! The last thing we need is a way to query and understand which users are part of a conversation and which conversations a user is part of. For that, I'm going to add one more table here that I'm going to call conversation users, and it's just going to store this mapping from a conversation ID to our user ID. The idea is that there'd be a row here for every user in a conversation, and we'd be able to index these in order to do queries like we were just talking about.

Okay, great! Now that we've talked about how we're going to store data in our database, let's revisit our overall architecture for a moment and think about how we can make the system more scalable and more performant. In particular, one thing I'm thinking about is the cost of going to our database and retrieving messages from it repeatedly. One thing I'd like to add here is some sort of caching service or caching layer, which would be like a read-through cache that we can store in memory so that we don't always have to go to our database and fetch messages from it directly. 

I'm just going to draw that in here. Now, another thing we talked about but didn't really discuss very much is how we're going to store media—images and videos—and how we can upload those to the correct place. Now, we're not going to store those in our database, but instead, we're going to choose some sort of other storage platform, like an object storage service like Amazon S3. I'm going to go ahead and draw that in up here. The idea is that when our API receives a request to upload some content, we'll actually just forward it on to this object storage system, and then we'll store the URL of that object alongside our message, as we discussed before. 

On the user side, when you receive a message that contains some sort of media, you'll then go and fetch that URL separately in order to download it. To make that more efficient, we'll also want to add some sort of caching here, and in this case, we would use something like a CDN. The idea is that the user would request the resource from the CDN, and if it's cached already, that's great. If it's not, the CDN would request that object from the object storage service in case there's a cache miss.

All right, great! Now, the last thing we want to add here is some sort of way to notify users who are offline about messages they may have missed. In this case, we might want to have some sort of notification service here that's also going to be contacted by our message service in the event that the user is offline. In this case, our message service will contact our notification service, and our notification service will forward that notification to the user, probably via some sort of third-party API for iOS devices or Android devices, or perhaps even through some sort of mail service.

All right, we've covered a lot of ground, and we've talked about everything from choosing the right network protocol for our clients to building a distributed messaging queue system in our backend and picking the right database and data model to store all these messages. I think with that, we've covered pretty much everything we need to talk about. While we could go more in depth on each of these topics, I think hopefully you have the big picture and the overview of how we would build this application and have learned some of the important technical decisions and trade-offs that we need to be thinking about when we implement something like this.