"[Music] Inside a nondescript building in the heart of San Francisco, one of the world's buzziest startups is making our AI-powered future feel more real than ever before. They're behind two monster hits, ChatGPT and DALL-E, and somehow beat the biggest tech giants to market, kicking off a competitive race that's forced them all to show us what they've got. But how did this under-the-radar startup pull it off? We're inside OpenAI, and we're going to get some answers. Is it magic? Is it just algorithms? Is it going to save us or destroy us? Let's go find out. 

I love the plants; it feels so alive, so amazing. It's giving me very Westworld spa vibes. It's almost like suspended in space and time a little bit. Yeah, it has a little bit of a futuristic feel. 

This is one of the most introspective minds at OpenAI. We all know Sam Altman, the CEO, but Mira Murati is the chief architect behind OpenAI's strategy. 

This looks like the OpenAI logo. 

It is! Ilya actually painted this earlier, the chief scientist. 

Yes, what is the flower meant to symbolize? 

My guess is that it's AI that loves humanity. 

We're very focused on dealing with the challenges of hallucination, truthfulness, reliability, and alignment of these models. 

Has anyone left because they're like, 'You know what, I disagree?' 

There have been, over time, people that left to start other organizations because of disagreements on the strategy around deployment. 

And how do you find common ground when disagreements do arise? 

You know, you want to be able to have this constant dialogue and figure out how to systematize these concerns. 

What is the job of a CTO? 

It's a combination of guiding the teams on the ground, thinking about longer-term strategy, figuring out our gaps, and making sure that the teams are well supported to succeed. 

Yeah, sounds like a big job. 

Solving impossible problems. 

So when you were making the decision about releasing ChatGPT into the wild, I'm sure there was a go or no-go moment. Take me back to that day. 

We had ChatGPT for a while, and we sort of hit a point where we could really benefit from having more feedback on how people are using it, what are the risks, what are the limitations, and learn more about this technology that we have created and start bringing it into the public consciousness. 

It became the fastest growing tech product in history. 

Did that surprise you? I mean, what was your reaction to the world's reaction? 

We were surprised by how much it captured the imaginations of the general public and how much people just loved spending time talking to this AI system and interacting with it. 

It can now mimic a human; it can write, it can code. At the most basic level, how does this all happen? 

It's a neural network that has been trained on a huge amount of data on a massive supercomputer, and the goal during this training process was to predict the next word in a sentence. It turns out that as you train larger and larger models and add more and more data, the capabilities of these models also increase. They become more powerful, more helpful, and as you invest more in alignment and safety, they become more reliable and safe over time. 

OpenAI has kind of turbocharged this competitive frenzy. Do you think you can beat Google at its own game? Do you think you can take significant market share in search? 

We didn't set out to dominate search. What ChatGPT offers is a different way to understand information. You could be searching, but you're searching in a much more intuitive way versus keyword-based. I think the whole world is sort of now moving in this direction. 

The air of confidence, obviously, that ChatGPT sometimes delivers an answer with. Why not just sometimes say, 'I don't know?' 

The goal is not to predict the next word reliably or safely. When you have such general capabilities, it's very difficult to handle some of the limitations, such as what is correct. Some of these texts and some of the data are biased; some of it may be incorrect. 

Isn't this going to accelerate the misinformation problem? I mean, we haven't been able to crack it on social media for like a couple of decades. 

Misinformation is a really complex problem right now. One of the things that I'm most worried about is the ability of models like GPT-4 to make up things. We refer to this as hallucinations. They will convincingly make up things, and it requires being aware and just really knowing that you cannot fully blindly rely on what the technology is providing as an output. 

I want to talk about this term hallucination because it's a very human term. Why use such a human term for basically an AI that's just making mistakes? 

A little bit. General capabilities are actually quite human-like. Sometimes when we don't know the answer to something, we will just make up an answer. We will rarely say, 'I don't know.' So there is a lot of human hallucination in a conversation, and sometimes we don't do it on purpose. 

Should we be worried about AI, though, that feels more and more human-like? Should AI have to identify itself as artificial when it's interacting with us? 

I think it's a different kind of intelligence. It is important to distinguish output that's been provided by a machine versus another human. But we are moving towards a world where we are collaborating with these machines more and more, and so output will be hybrid. 

All of the data that you're training this AI on is coming from writers; it's coming from artists. How do you think about giving value back to those people when these are also people who are worried about their jobs going away? 

I don't know exactly how we could work in practice that you can sort of account for information created by everyone on the internet. I think there are definitely going to be jobs that will be lost and jobs that will be changed as AI continues to advance and integrate into the workforce. 

Prompt engineering is a job today; that's not something that we could have predicted. Think of prompt engineers like AI whispers. They're highly skilled at selecting the right words to coax AI tools into generating the most accurate and illuminating responses. It's a new job born from AI that's fetching hundreds of thousands of dollars a year. 

What are some tips to being an ace prompt engineer? 

You know, it's this ability to really develop an intuition for how to get the most out of the model—how to prompt it in the right ways, give it enough context for what you're looking for. One of the things that we talked about earlier was hallucinations and these large language models not having the ability to always be highly accurate. 

So I'm asking the model with a browsing plugin to fact-check this information, and it's now browsing the web. 

So there's this report that these workers in Kenya were getting paid two dollars an hour to do the work on the backend to make answers less toxic. My understanding is this work can be difficult, right? Because you're reading text that might be disturbing and trying to clean it up. 

So we need to use contractors sometimes to scale. We chose that particular contractor because of their known safety standards, and since then we've stopped working with them. But as you said, this is difficult work, and we recognize that. We have mental health standards and wellness standards that we share with contractors. 

I think a lot about my kids and them having relationships with AI someday. How do you think about what the limits should be and what the possibilities should be when you're thinking about a child? 

I think we should be very careful in general with putting very powerful systems in front of more vulnerable populations. There are certainly checks and balances in place because it's still early, and we still don't understand all the ways in which this could affect people. 

There's all this talk about relationships and AI. Could you see yourself developing a relationship with an AI? 

I'd say yes. It's a reliable tool that enhances my life and makes my life better. 

As we ponder the existential idea that we might all have relationships with AI someday, there's an AI gold rush happening in Silicon Valley. Venture capitalists are pouring money into anything AI startups, hoping to find the next big thing. 

Reid Hoffman, the co-founder of LinkedIn and an early investor in Facebook, knows a thing or two about striking gold. He was an early OpenAI backer and is, in a way, trying to take society's hand and guide us all through the age of AI. 

I mean, gosh, 12 years we've been talking—maybe longer. 

That's awesome. 

A long time. Yes, you have been on the ground floor of some of the biggest tech platform shifts in history: the beginnings of the internet, mobile. Do you think AI is going to be even bigger? 

I think so. It builds on the internet, mobile, cloud, data—all of these things come together to make AI work, and so that causes it to be the crescendo, the addition to all of us. 

I mean, one of the problems with the current discourse is that it's too much fear-based versus hope-based. Imagine a tutor on every smartphone for every child in the world—that's possible. That's in line with what we see with current AI models today. 

You coined this term "blitzscaling." Blitzkilling, in its precise definition, is prioritizing speed over efficiency in an environment of uncertainty. How do you go as fast as possible in order to be the first to scale? 

Does it today? 

Does it! 

I think the speed at which we will integrate it into our lives will be faster than we integrated the iPhone into our lives. There's going to be a co-pilot for every profession. And if you think about that, that's huge—not just for professional activities because it's going to write my kids' papers. 

Right, right. 

It's high school papers. 

Uh yes, although the hope is that in the interaction with it, they'll learn to create much more interesting papers. 

You and Elon Musk go way back. He co-founded OpenAI with Sam Altman, the CEO of OpenAI. You and I have talked a lot over the years about how you have been sort of this node in the PayPal Mafia, and you can talk to everyone. And maybe you disagree, but you are all still friends. What did Elon say that got you interested so early? 

Part of the reason I got back into AI, and I was part of sitting around the table in the crafting of OpenAI, was that Elon came in and said, "Look, this AI thing is coming." Once I started digging into it, I realized that this pattern that we're going to see—the next generation of amazing capabilities coming from these computational devices—and then one of the things I had been arguing with Elon at the time about was that Elon was constantly using the word "robocalypse," which we as human beings tend to be more easily and quickly motivated by fear than by hope. 

So you're using the term "robocalypse," and everyone imagines the Terminator and all the rest. Sounds pretty scary. 

It sounds very scary. 

It doesn't sound like something we want. 

Yeah, stop saying that because actually, the chance that I could see anything like a robocalypse happening is so de minimis relative to everything else. 

So how did you come together on OpenAI? How did that happen? 

I think it started with Elon and Sam having a bunch of conversations. Since I know both of them quite well, I got called in, and I was like, "Look, I think this could really make sense. Something should be the counterweight to all of the natural work that's going to happen within commercial realms. How do we make sure that one company doesn't dominate the industry, but the tools are provided across the industry so innovation can benefit from startups and all the rest?" It was like, "Great, let's do this thing—OpenAI." 

I did ask ChatGPT what questions I should ask you. I thought its questions were pretty boring. 

Yes, your answers were pretty boring too, so we're not getting replaced anytime soon. But clearly, this has really struck a nerve. There are people out there who are going to fall for it. 

Yes, shouldn't we be worried about that? 

Okay, so everyone's encountered a crazy person who's drunk off their ass at a cocktail party who says really odd things, or at least every adult has. And you know, that's not like the world didn't end. 

Right. 

We do have to pay attention to areas that are harmful. Like, for example, if someone's depressed, the thing about self-harm, you want all channels by which they could get into self-harm to be limited. That isn't just chatbots; that could be communities of human beings; that could be search engines. You have to pay attention to all the dimensions of it. 

How are we overestimating AI? 

It still doesn't really do something that I would say is original to an expert. So for example, one of the questions I asked was, "How would Reid Hoffman make money by investing in artificial intelligence?" And the answer it gave me was a very smart, very well-written answer that would have been written by a professor at a business school who didn't understand venture capital. 

Right. 

So it seems smart, but it would study large markets, would realize what products would be substituted in the large markets, would find teams to go do that, and invest in them. And this is all written very credibly and completely wrong. 

The newest edge of the information is still beyond these systems. Billions of dollars are going into AI. My inbox is filled with AI pitches. Last year it was crypto and Web3. How do we know this isn't just the next bubble? 

I do think that generative AI is the thing that has the broadest touch of everything now. 

Which places are the right places to invest? 

I think those are still things we're working out. Now, obviously, as venture capitalists, part of what we do is we kind of figure that out in advance, you know, years before other people see it coming. But I think that there will be massive new companies built. 

It does seem, in some ways, like a lot of AI is being developed by an elite group of companies and people. Is that something that you see happening? 

In some ideal universe, you'd say for a technology that would impact billions of people, somehow billions of people should directly be involved in creating it. But that's not how any technology anywhere in history gets built. And there are reasons you have to build it at speed. 

But the question is, how do you get the right conversations and the right issues on the table? 

So do you see an AI Mafia? 

For me, I definitely think that there is—because you're referring to the PayPal Mafia, of course. I definitely think that there's a network of folks who have been deeply involved over the last few years that will have a lot of influence on how the technology happens. 

Do you think AI will shake up the big tech hierarchy significantly? 

What it certainly does is create a wave of disruption. For example, with these large language models in search, what do you want? Do you want 10 blue links, or do you want an answer? In a lot of search cases, you want an answer, and a generated answer that's like a mini Wikipedia page is awesome. That's a shift. 

So I think we'll see a profusion of startups doing interesting things, but can the next Google or Facebook really emerge if Google, Facebook, or Meta, Apple, and Amazon are running the playbook at Microsoft? 

Do I think there will be another one to three companies that will be the size of the five big tech giants emerging from AI? Absolutely, yes. Now, does that mean that one of them is going to collapse? No, not necessarily, and it doesn't need to. The more that we have, the better. 

So what are the next big five? 

Oh well, that's what we're trying to invest in. 

You're on the board of Microsoft. Obviously, you know Microsoft is making a big AI push. Did you bring Satya and Sam or have any role in bringing Satya and Sam closer together? Because Microsoft obviously has 10 billion dollars now in OpenAI. 

Well, I think I could. I probably have a, you know, both of them are close to me and know me and trust me well. So I think I've helped facilitate understanding and communications. 

Elon left OpenAI years ago and pointed out that it's not as open as it used to be. He said he wanted it to be a non-profit counterweight to Google. Now it's a closed-source maximum profit company effectively controlled by Microsoft. Does he have a point? 

Well, he's wrong on a number of levels there. So one is, it's run by a 501(c)(3); it is a non-profit, but it does have a for-profit part. The commercial system, which is all carefully done, is to bring in capitalism to support the non-profit mission. 

Now, get to the question of, for example, OpenAI. So, DALL-E was ready for four months before it was released. 

Why the delay for four months? 

Because it was doing safety training. It said, "Well, we don't want to have this being used to create child sexual material. We don't want to have this being used for assaulting individuals or doing deepfakes. So we're not going to open source it; we're going to release it through an API so we can be seeing what the results are and making sure it doesn't do any of these harms." So it's open because it has open access to APIs, but it's not open because it's open source. 

There are folks out there who are angry, actually, about OpenAI's branching out from non-profit to for-profit. Is there a bit of a bait and switch there? 

The cleverness that Sam and everyone else figured out is they could say, "Look, we can do a market commercial deal where we say we'll give you commercial licenses to parts of our technology in various ways, and then we can continue our mission of beneficial AI." 

The AI graveyard is filled with algorithms that got into trouble. How can we trust OpenAI or Microsoft or Google or anyone to do the right thing? 

Well, we need to be more transparent, but on the other hand, of course, our problem—exactly as you're alluding to—is people say, "Well, the AI should say that or shouldn't say that." We can't even really agree on that ourselves, so we don't want that to be litigated by other people. We want that to be a social decision. 

So how does this shake out globally? 

We should be trying to build the industries of the future; that's what's the most important thing. And it's one of the reasons why I tend to very much speak against people like, "Oh, we should be slowing down." 

Do you have any intention of slowing down? 

We've been very vocal about these risks for many, many years. One of them is acceleration, and I think that's a significant risk that we as a society need to grapple with. Building safe AI systems that are general is very complex; it's incredibly hard. 

So what does responsible innovation look like to you? 

You know, like, would you support, for example, a federal agency like the FDA that oversees technology like it does drugs? Having some sort of trusted authority that can audit these systems based on some agreed-upon principles would be very helpful. 

I've heard AI experts talk about the potential for the good future versus the bad future. In the bad future, there's talk about this leading to human extinction. Are those people wrong? 

There's certainly a risk that when we have these AI systems that are able to set their own goals, they decide that their goals are not aligned with ours, and they do not benefit from having us around and could lead to human extinction. That is a risk. I don't think this risk has gone up or down from the things that have been happening in the past few months. I think it's certainly been quite hyped, and there is a lot of anxiety around it. 

If we're talking about the risk for human extinction, have you had a moment where you're just like, "Wow, this is big?" 

I think a lot of us at OpenAI enjoy it because we thought that this would be the most important technology that humanity would ever create. But of course, the risks on the other hand are also pretty significant, and this is why we're here. 

Do OpenAI employees still vote on AGI and when it will happen? 

I actually don't know. 

What is your prediction about AGI and how far away it really is? 

We're still quite far away from being at a point where, you know, these systems can make decisions autonomously and discover new knowledge. But I think I have more certainty around the advent of having powerful systems in our future. 

Should we even be driving towards AGI, and do humans really want it? 

Advancements in society come from pushing human knowledge. Now, that doesn't mean that we should do so in careless and reckless ways. I think there are ways to guide this development versus bringing it to a screeching halt because of our potential fears. 

So the train has left the station, and we should stay on it. 

That's one way to put it. 

[Music] [Music]"