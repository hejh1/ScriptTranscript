How would you design a real-time messaging app that can support millions of users? In this video, we'll be talking about how to design and build an application like Facebook Messenger, WhatsApp, Discord, or Slack. We'll be talking about the high-level architecture of these systems, as well as some specific features like how to build real-time messaging, group messaging, image and video uploads, as well as push notifications. Now, we'll be talking about some best practices as well along the way about how
to build and scale a reliable system. Now, before we dive into some specific features and components of our system, let's talk a little bit about some of the high-level features and goals that we want to implement along the way. Now, I think some important product goals that we want to support obviously are real-time messaging from one individual to another, as well as group messaging that can support multiple users. We want to support maybe something like an online status
is currently online or offline, as well as supporting image and video uploads in addition to text messages. And we also may want to support some extra bells and whistles like read receipts or push notifications. So we'll get to those if we have time. And then we also want to keep in mind some technical goals and constraints here. So thinking about how to build a low latency system to support real-time messaging and what applications or technologies need to sort of choose to make that happen. We also want to build something that can
support a really high volume of requests. So millions of users potentially writing messages at the same time. We also want to make sure, of course, that our system is highly reliable and available. And we also want to make sure that our messaging system is secure and doesn't result in users being sent messages they shouldn't receive. All right, great. So now that we've laid out some of the features we want to support, let's talk about the overall architecture of this app. And specifically, let's talk about how we're going to send messages from one user.
to another. So, you know, let's pretend we have two users here and we want to send a message between them. So, the reason we need to use a chat server application in this case is because on the internet it's actually really hard to establish a direct connection from one user to another and to make that connection reliable. And in addition, we also want to support things like storing and retrieving message histories later. So, we need to have our chat server here in order to sort of establish and broker this connection and to sort of store these messages
or treat from different devices later in time. So I'm going to go ahead and draw in our chat API server. And then we also need to sort of establish a connection from users to this chat server. Now let's think about what happens when user A sends a message to user B. What we want to happen is the user sends a message to the server, and the server relays that message instantly to the user that it's intended for. However, this kind of breaks the model of how HTTP requests work on the internet because
They can't actually be server initiated, they have to be client initiated. So something about this isn't going to work, and we're going to have to come up with something else. Now, I've got a few options in mind, so I'm going to talk about those and discuss their trade-offs. The first one we can do is something called HTTP polling. And in this model, instead of just sending one request to the server, we're going to sort of repeatedly ask the server if there's any new information available. And so most of the time, the server is going to reply with no, There's no new information available.
And then once in a while, it'll say, hey, yeah, I received a new message for you, and it will reply with that. For a variety of reasons, this is probably not the right solution for this problem, because it means we're going to be sending a lot of unnecessary requests to our server. And it also means that we're going to have a pretty high latency. So we're only going to be able to receive messages when we ask for them, not when they're actually received by the server. Now, the second option we have is something called long polling. And in this model, we're still using sort of a traditional
HTTP request, but instead of resolving immediately with the result, we're actually going to have the server hold on to the request and wait until data is available before it replies with the result. So in this way, we sort of maintain an open connection with the server at all times, and then once data is sent back, we immediately request a new connection, and then we keep that open until data is available. Now, this is a little bit better because it solves our latency problem somewhat and we don't have to create all these unnecessary requests all the time.
However, we have to maintain this open connection. And if there's lots of data coming from the server, it means we still have to initiate a new request to get the next piece of data. So while it's good for some systems, like notifications and things like that, it's probably not the best for a real-time chat application like we're trying to build here. So the third option we have is something called Web Sockets. And this is the solution I'm going to recommend because it was sort of designed for this application. Now in Web Sockets, we still maintain an open connection
server, but instead of being just a one-way connection, it's actually a full duplex connection. So now we can send up data to the server and the server can push down data to us and this connection is maintained and kept open for the duration of the session. So this architecture presents some unique challenges for us because there's some practical limitations to how many open connections a server can have at one time. WebSockets is built on the TCP protocol which has about 16 bits for the port number.
there's a real limitation of about 65,000 connections that any one server can have open at a time. So instead of having one API server, we're obviously going to need to have a lot of servers to handle all of these WebSocket connections. And we're going to need a load balancer or gateway sitting in front of them to help balance these connections and route them to the correct server. So I'm going to go ahead and replace this with a load balancer. And I'm going to dry in some API servers here instead.
>> All right, great. So I'm just sort of visualizing this with about three servers, but if we're trying to support millions or hundreds of millions of users, and we can only support thousands of requests per server, that means we're going to have hundreds or thousands of these servers serving requests and keeping these connections open. So our system is going to have to reach a pretty massive scale.
here. And in addition, we now have a new problem because before we were able to sort of send a message from one user to another via our chat API server, however, now we have a distributed system, and we need to be able to communicate from one API server to another. And in fact, we've actually created sort of another messaging problem here because these API servers need to know how to talk to each other. So one model we could adopt here, one design pattern we can use is something like
message queue. And so that fits nicely into this problem because it's sort of a natural solution for a messaging problem between servers in a distributed system. So I'm going to go ahead and draw in a message service here, which is going to sort of implement this message queue.
The idea here is that each API server will publish messages into this centralized queue and subscribe to updates for the users that it's connected to. That way, when a new message comes in, it can be added to the queue, and any service that's listening for messages for that user can then receive that update and forward the message onto the user. So that's how that would work. Now, we still need to think about how we're going to store and persist these messages in our database, and how to sort of model this
relationship between messages and users. So let's go ahead and draw in a database here and think about how that's going to work. Now when we think about what kind of database to choose for application, let's think back to the beginning where we set out our requirements for this system. So we know we want to support a really large volume of requests and store a lot of messages.
a lot about the availability and uptime of our service. So we want to pick a database that's going to fit these requirements. And we know from things like the cap theorem that there's going to be the sort of universal trade-offs between principles like consistency, availability, and ability to partition or shard our database. And so we want to focus on this ability to shard and partition and keep our database available rather than things like consistency, which are less important in a messaging application, then they would be in something like a financial app.
application. So with that in mind, I think I would choose something like a no-sequel database that has built-in replication and sharding abilities. So something like Cassandra or HBase would be great for this application. All right, so now let's talk about how we're going to store and model this data in our database. We know we're going to need a few key tables and features like users, messages. And then we're also going to probably need this concept of conversations, which will be groups of users who are supposed
So I'm going to go ahead and draw those tables in. So starting with the user's table, we're going to have a unique ID for this user. We're also going to have something like a user name or a name for them to display. And then we're probably also going to want to have something like a last active timestamp. And the idea here is this sort of allows to support features like online status or being able to see when a user was last online. And this would just be a simple timestamp
the date of their last activity. All right, so next we're going to need a messages table. And again, this is going to have a unique ID. But it's also going to store a reference to the user who made the message. And we're also going to have a reference or an ID for the conversation that it belongs to. We'll talk more about that in a moment. But then obviously at the end, we're also going to need the text of the message as well.
And if we want to support something like media uploads, like images or videos, we also want to store something like a media URL here in our database as well. This won't be the actual data, but it'll be the URL where the user can access this data to download it. All right. So next, we're going to need this conversations table that I was talking about. And the idea here is that this would simply just be an ID and perhaps something like a name.
In the case of an application like Slack or Discord, this could be the channel name of the conversation. So that's sort of an optional string that we could store here. All right, great. So the last thing we need is a way to query and understand which users are part of a conversation and which conversations a user is part of. So for that, I'm going to add one more table here that I'm going to call conversation users. And it's just going to store this sort of mapping from a conversation ID.
to our user ID. So the idea is that there'd be a row here for every user in a conversation, and we'd be able to index these in order to do queries like we were just talking about. Okay, great. So now that we've talked about how we're going to store data in our database, let's revisit our overall architecture for a moment and think about how we can make the system more scalable and more perform it. So in particular, one thing I'm thinking about is the cost of
and retrieving messages from it repeatedly. So one thing I'd like to add here is some sort of caching service or caching layer, which would be like a read through cache that we can store in memory so that we don't always have to go to our database and fetch messages from it directly. So I'm just going to draw that in here.
Now, another thing we talked about but didn't really discuss very much is how we're going to store media. So images and videos and how we can upload those to the correct place. Now, we're not going to store those in our database, but instead we're going to choose some sort of other storage platform, like an object storage service like Amazon S3. So, I'm going to go ahead and draw that in up here. And the idea is that when our A
API receives a request to upload some sort of content. We'll actually just forward it onto this object storage system, and then we'll store the URL of that object alongside our message as we discussed before. On the user side, when you receive a message that contains some sort of media, you'll then go and fetch that URL separately in order to download it. Now, in order to make that more efficient, we'll also want to add some sort of caching here. And in this case, we would use something like a CDN. And the idea is that the user would request.
request the resource from the CDN. And if it's cashed already, that's great. And if it's not, the CDN would request that object from the object storage service in case there's a cash miss.
All right, great. The last thing we want to add here is some sort of way to notify users who are offline about messages they may have missed. So in this case, we might want to have some sort of notification service here that's also going to be contacted by our message service in the event that the user is offline. So in this case, our message service will contact our notification service and our notification service will forward that notification onto the user.
via some sort of third party API for iOS devices or Android devices or perhaps even through some sort of mail service. All right, we've covered a lot of ground and we've talked about everything from choosing the right network protocol for our clients to building a distributed messaging queue system in our back end and picking the right database and data model to store all these messages. I think with that, we've covered pretty much everything we need to talk about. And while we could go more in depth on each of these topics, I think hopefully you have
the big picture and the overview of how we would build this application and have learned some of the important technical decisions and trade-offs that we need to be thinking about when we implement something like this.
